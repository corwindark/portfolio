---
title: "Metalearning in Time Series Ensembles"
subtitle: "Using Deep Learning to Select Statistical Models"
date: "2023-09-01"
author: "Corwin Dark"
image: "image.jpg"
---



Get Hourly SPY Data Since 1/1/22
```{python}
import yfinance as yf
import pandas as pd
import seaborn as sns
import matplotlib as plt
from sklearn.model_selection import TimeSeriesSplit
from datetime import datetime
import matplotlib.pyplot as plt




data = yf.download('SPY','2022-01-01','2023-09-30', interval ="60m")
%matplotlib inline

data['Adj Close'].plot()
plt.ion()
print(data)

```


Now we need to split the data.

``` {python}
data.reset_index(inplace = True)

```


```{python}
train_size = 2400

data_train = data[:train_size]
data_test = data[train_size:]

test_size = len(data_test)


data_test = data_test[["Datetime", "Close"]]
data_train = data_train[["Datetime", "Close"]]

seriesTrain = data_train.set_index('Datetime')
seriesTest = data_test.set_index('Datetime')


```


Attempting to implement with sklearn tscv 
```{python}
"""
tscvData = data[["Close"]]


# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html

tscv = TimeSeriesSplit(gap=0 max_train_size=None, n_splits=5, test_size=1)

for i, (train_index, test_index) in enumerate(tscv.split(tscvData)):
    print(i, "train: ", train_index, "test: ", test_index)
    model = auto_arima(tscvData[train_index])
    predictions = model.predict(n_periods = 1)
    print(predictions)
"""

```


``` {python}
# Let's reformat the test data into a dataframe we can add our predictions to

testResultsDF = pd.DataFrame(index = range(test_size))
testResultsDF['Price'] = 0
# Get a clean format for close prices
for i in range(0,test_size):
    testResultsDF['Price'][i] = seriesTest.values[i][0]


# Initialize empty cells for the statistical forecasts
testResultsDF['auto_arima'] = 0
testResultsDF['dyn_theta'] = 0
testResultsDF['auto_ets'] = 0
testResultsDF['complex_smoothing'] = 0

```


``` {python}
def crossValidation(train, test, window, outputFrame, modelName, predReturnFunction):

    totalPredictions = pd.Series([])
    windows = len(test) // window

    predictionIndex = 0

    for i in range(0, windows):
        # debug
        print("Window: ", i, "Method: ", modelName)

        # How many observations to move forward each frame
        addNum = i * window
        # Combine training data with additional test window
        intermediateData = pd.concat([train, test[:addNum]])
        # Generate prediction of the given window size
        prediction = predReturnFunction(intermediateData, window)
        print(prediction)
        # Check if we have multiple predictions in the window
        if len(prediction) > 1:
            # Store each predicted value with a loop
            for j in range(0,window):

                # Store in the prediction-comparison frame
                outputFrame[modelName][predictionIndex] = prediction[j]

                # Move to next open spot
                predictionIndex += 1 
        else:
            
            outputFrame[modelName][predictionIndex] = prediction[0]
            predictionIndex += 1




```

And then we need to train the statistical models


Import packages
```{python}
from statsforecast import StatsForecast
from statsforecast.models import AutoCES, AutoARIMA, AutoETS, DynamicOptimizedTheta
import numpy as np
import time
```

Run Prediction
```{python}

def aaPredFunction(dataIn, windowSize):
    model = AutoARIMA()
    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))
    prediction = fit1.predict(h = windowSize)
    return prediction.get('mean')


def cesPredFunction(dataIn, windowSize):
    model = AutoCES()
    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))
    predictionList = fit1.predict(windowSize)
    return predictionList.get('mean')

def etsPredFunction(dataIn, windowSize):
    model = AutoETS()
    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))
    predictionList = fit1.predict(windowSize)
    return predictionList.get('mean')

def dotPredFunction(dataIn, windowSize):
    model = DynamicOptimizedTheta()
    fit1 = model.fit(y = np.concatenate(dataIn.to_numpy()))
    predictionList = fit1.predict(windowSize)
    return predictionList.get('mean')


# Set the number of observations to be included in the test set
test_out_size = 40

tempTest = seriesTest[:test_out_size]

# Auto Arima
crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'auto_arima', aaPredFunction)

crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'complex_smoothing', cesPredFunction)

crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'auto_ets', etsPredFunction)

crossValidation(seriesTrain, tempTest, 1, testResultsDF, 'dyn_theta', dotPredFunction)


```


Let's try and visualize the cross-validated auto-arima performance

``` {python}

print(testResultsDF)

plot1 = testResultsDF.iloc[0:test_out_size,:]

# get the index as a column for plotting
plot1 = plot1.reset_index()

plot1 = pd.melt(plot1, id_vars = ['index'], value_vars =  ['auto_arima', 'auto_ets', 'complex_smoothing', 'dyn_theta', 'Price'])

print(plot1)

sns.lineplot(plot1, x = 'index', y = 'value', hue = 'variable')


```

Let's get a more detailed plot of how close each method is at each increment

``` {python}

print(testResultsDF)

plot2 = testResultsDF.iloc[0:test_out_size,:]

plot2['auto_arima'] = plot2['auto_arima'] - plot2['Price']
plot2['auto_ets'] = plot2['auto_ets'] - plot2['Price']
plot2['complex_smoothing'] = plot2['complex_smoothing'] - plot2['Price']
plot2['dyn_theta'] = plot2['dyn_theta'] - plot2['Price']





# get the index as a column for plotting
plot2 = plot2.reset_index()

errorDat = plot2.copy()

plot2 = pd.melt(plot2, id_vars = ['index'], value_vars =  ['auto_arima', 'auto_ets', 'complex_smoothing', 'dyn_theta'])

sns.barplot(plot2, x = 'index', y = 'value', hue = 'variable')


```


Let's look at cumulative absolute error of the models, compared to best choice at each time point

```{python}


errorDat['optimal'] = 0

for index in range(0, errorDat.shape[0]):
    print(index)
    predictionErrors = errorDat.loc[:,('auto_arima', 'auto_ets', 'dyn_theta', 'complex_smoothing') ]
    
    print(predictionErrors.min())

    errorDat.loc[index ,'optimal'] = min(predictionErrors)
   
errorDat

```



backN = 50

smallTest = seriesTest[:test_out_size]
smallTrain = seriesTrain[-backN:]

smallTest["x"] = range(backN + 1,backN + test_out_size + 1)
smallTrain["x"] = range(1,backN + 1)

#aaPrediction = pd.Series(aaPrediction)
#aaPrediction.index = smallTest.index
#aaPrediction = pd.Series(aaPrediction)
#aaPrediction.index = smallTest.index
aaPrediction2 = pd.DataFrame([float(x) for x in aaPrediction])
aaPrediction2["x"] = range(backN + 1, backN + test_out_size + 1)
aaPrediction2.rename(columns = {0 : "val", "x": "x"}, inplace = True )
aaPrediction2 = pd.DataFrame(aaPrediction2)

print(aaPrediction)

plt.figure(figsize= (8,5) )
plt.plot(smallTrain["x"], smallTrain["Close"], label = "Training")
plt.plot(smallTest["x"], smallTest["Close"], label = "Test")
plt.plot(aaPrediction2["x"], aaPrediction2["val"], label = "Auto Arima")
plt.legend(loc = 'upper left')
plt.show()



And then we can reformat data for NN









